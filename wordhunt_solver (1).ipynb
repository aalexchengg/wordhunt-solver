{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Su3zW5IEm3P2"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from itertools import product\n",
        "import math\n",
        "import io\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PREPROCESSING\n",
        "def get_ngrams(path, n):\n",
        "  result = []\n",
        "  with open(path) as input:\n",
        "    for line in input:\n",
        "      if(len(line)<3 or len(line)>16):\n",
        "        continue\n",
        "      line = \"0\" + line\n",
        "      for i in range(len(line) - (n-1)):\n",
        "        result.append((line[i:i+n]))\n",
        "  return result\n",
        "\n",
        "get_ngrams(\"tester.txt\", 3)\n",
        "\n",
        "\n",
        "def post_process(input):\n",
        "  result = []\n",
        "  for value in input:\n",
        "    result.append(value[1:-1])\n",
        "  return result"
      ],
      "metadata": {
        "id": "ZUTujwufng2m"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HELPER FUNCTIONS\n",
        "\n",
        "#turns a 2d matrix into an adjacency list of indeces\n",
        "def board_to_adjlist(board):\n",
        "    result = dict()\n",
        "    for i in range(len(board)):\n",
        "        for j in range(len(board[0])):\n",
        "            idx = (i,j)\n",
        "            neighbors = []\n",
        "            for x in range(-1,2):\n",
        "                for y in range(-1,2):\n",
        "                    if(not(x == 0 and y ==0)):\n",
        "                        if(x + i >=0 and y + j >=0 and x + i < len(board) and y + j < len(board[0])):\n",
        "                            neighbors.append((i+x,j+y))\n",
        "            result[idx] = neighbors\n",
        "    return result\n",
        "\n",
        "#turns a 2d matrix into a list of characters\n",
        "def board_to_list(board):\n",
        "  result = []\n",
        "  for row in board:\n",
        "    result = result + row\n",
        "  result.sort()\n",
        "  return result\n",
        "\n",
        "def board_to_dict(board):\n",
        "  result = dict()\n",
        "  for i in range(len(board)):\n",
        "    for j in range(len(board[0])):\n",
        "      if(board[i][j] not in result):\n",
        "        result[board[i][j]] = [(i,j)]\n",
        "      else:\n",
        "        result[board[i][j]].append((i,j))\n",
        "  return result\n",
        "\n",
        "def exists(word, chars):\n",
        "  #REQUIRES: char is sorted\n",
        "  word = list(word)\n",
        "  word.sort()\n",
        "  curr = 0\n",
        "  i = 0\n",
        "  length = len(chars)\n",
        "  goal = len(word)\n",
        "  while(i < length):\n",
        "    if(curr == goal):\n",
        "      return True\n",
        "    if(word[curr] == chars[i]):\n",
        "      curr +=1\n",
        "    elif(word[curr] < chars[i]):\n",
        "      return False\n",
        "    i +=1\n",
        "  return curr == goal \n",
        "\n",
        "def validate(dictionary, result):\n",
        "  res = []\n",
        "  for word in result:\n",
        "    num = hash(word) % dictionary.size\n",
        "    if (num in dictionary.book) and (word in dictionary.book[num]):\n",
        "      res.append(word)\n",
        "  return res "
      ],
      "metadata": {
        "id": "f6In1_lC07Gj"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Literal Hash Dictionary"
      ],
      "metadata": {
        "id": "J0OOoly0Gvxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dictionary():\n",
        "  def __init__(self, path, size):\n",
        "    self.size = size\n",
        "    self.book = self.build(path, size)\n",
        "    self.listed = self.to_list(path)\n",
        "  \n",
        "  def build(self, path, size):\n",
        "    result = dict()\n",
        "    with open(path) as input:\n",
        "      for line in input:\n",
        "        line = line[:-1]\n",
        "        num = hash(line) % size\n",
        "        if num in result:\n",
        "          result[num].append(line)\n",
        "        else:\n",
        "          result[num] = [line]\n",
        "    return result\n",
        "  \n",
        "  def to_list(self, path):\n",
        "    result = []\n",
        "    with open(path) as input:\n",
        "      for line in input:\n",
        "        result.append(line[:-1])\n",
        "    return result\n",
        "\n",
        "  def get_word(self, word):\n",
        "    num = hash(word) % self.size\n",
        "    return (num in self.book and word in self.book[num])\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ck0725JYObW-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Our N-Gram Language Model"
      ],
      "metadata": {
        "id": "SXL7xTibGoqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel():\n",
        "    def __init__(self, n, train_data, alpha=1):\n",
        "        \"\"\"\n",
        "        Language model class.\n",
        "        \n",
        "        Args\n",
        "        ____\n",
        "        n: int\n",
        "            n-gram order\n",
        "        train_data: List[List]\n",
        "            list of words in the dictionary that have not been preprocessed yet\n",
        "        alpha: float\n",
        "            Smoothing parameter\n",
        "        \n",
        "        Other required parameters:\n",
        "            self.vocab: vocabulary dict with counts\n",
        "            self.model: n-gram language model, i.e., n-gram dict with probabilties\n",
        "            self.n_grams_counts: Frequency count for each of the n-grams present in the training data\n",
        "            self.prefix_counts: Frequency count of all the corresponding n-1 grams present in the training data\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.train_data = train_data\n",
        "        self.tokens = self.train_data\n",
        "        self.n_grams_counts = None\n",
        "        self.prefix_counts = None\n",
        "        self.vocab  = Counter(self.tokens)\n",
        "        self.alpha = alpha\n",
        "        self.model = self.build()\n",
        "\n",
        "    def get_smooth_probabilities(self,n_gram):\n",
        "      vocab_size = len(self.vocab)\n",
        "      ngram_sum = self.n_grams_counts[n_gram]\n",
        "\n",
        "      prefix_sum = len(self.tokens)\n",
        "      if(self.n>1):\n",
        "        prefix_sum = self.prefix_counts[n_gram[0:self.n-1]]\n",
        "        \n",
        "        \n",
        "\n",
        "      return (ngram_sum + self.alpha)/(prefix_sum + self.alpha*vocab_size)\n",
        "\n",
        "\n",
        "      #Returns the smoothed probability of a single ngram, using Laplace Smoothing. Remember to handle the special case of n=1\n",
        "      #Use the class variables we defined in the build function. It is suggested to implement the build function before this one.\n",
        "\n",
        "    \n",
        "    #TODO:\n",
        "    def build(self):\n",
        "        \n",
        "        #Returns a n-gram (could be a unigram) dict with n-gram tuples as keys and probabilities as values. \n",
        "        #It could be a unigram model as well\n",
        "        ngrams = get_ngrams(self.train_data,n=self.n)\n",
        "        prefix_ngrams = get_ngrams(self.train_data,n = self.n-1)\n",
        "\n",
        "        self.n_grams_counts = Counter(ngrams)\n",
        "        self.prefix_counts = Counter(prefix_ngrams)\n",
        "\n",
        "        prob = dict()\n",
        "        for n_gram in self.n_grams_counts:\n",
        "          prob[n_gram] = self.get_smooth_probabilities(n_gram)\n",
        "\n",
        "        return prob"
      ],
      "metadata": {
        "id": "-YS-RbLznc5v"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Albert: N-Gram Language Model-Based Solver"
      ],
      "metadata": {
        "id": "pqlS4eMWGh8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Albert():\n",
        "  def __init__(self, model,n):\n",
        "    # I want to pass in a naive bayes n-gram model here.\n",
        "    #the probability \"p\" is the parameter we want to train\n",
        "    self.model = model\n",
        "    self.p = 0.03\n",
        "    self.n = n\n",
        "  \n",
        "\n",
        "  def bfs(self, tile, adjlist, board, path, result, visited):\n",
        "    #INPUT\n",
        "    #tile : the root tile that we are starting from\n",
        "    #adjlist : adjacency list of our graph\n",
        "    #board : 2d matrix of our graph so accessing is easier\n",
        "    #path : the current accumulated letters for our words\n",
        "    #pathprob : should we add the log probability of the current path? will do it on second try\n",
        "    #result : the resulting set from the past\n",
        "    #visited : the tiles that we have already visited\n",
        "\n",
        "    #OUTPUT\n",
        "    #result : the resulting set after conducting bfs\n",
        "\n",
        "    #check if current path is a word\n",
        "    finished = path[-(self.n-1):] + \"\\n\"\n",
        "    if(len(path) > 3 and finished in self.model and self.model[finished] >= self.p):\n",
        "      result.add(path + \"\\n\") \n",
        "\n",
        "    #check if extensions are a word\n",
        "    #for each neighbor\n",
        "    for neighbor in adjlist[tile]:\n",
        "        #if we haven't visited it yet, we do smth\n",
        "        #so if we've visited all the neighbors we just move on\n",
        "        if neighbor not in visited:\n",
        "            next = str(board[neighbor[0]][neighbor[1]])\n",
        "            candidate = path[-(self.n-1):] + next\n",
        "            #check probabilities\n",
        "            if(candidate in self.model and self.model[candidate] >= self.p):\n",
        "              new = path + next\n",
        "              temp = visited.copy()\n",
        "              temp.append(neighbor)\n",
        "              result = result | self.bfs(neighbor, adjlist, board, new, result, temp)\n",
        "    return result \n",
        "\n",
        "\n",
        "  def solve(self, board):\n",
        "      #INPUT\n",
        "      # board: a 2-d char array of n x n dimensions\n",
        "      # cutoff: the probability 0 < cutoff < 1, such that if the probability a character follows is lower than this cutoff we do not pursue this path\n",
        "\n",
        "      #OUTPUT\n",
        "      # result: a set of words found from the board\n",
        "\n",
        "      result = set()\n",
        "\n",
        "      #first, we want to make an adjacency list\n",
        "      adjlist = board_to_adjlist(board)\n",
        "      for tile in adjlist:\n",
        "        visited = [tile]\n",
        "        path = \"0\" + str(board[tile[0]][tile[1]])\n",
        "        found = self.bfs(tile, adjlist, board,path , set(), visited)\n",
        "        result = result | found\n",
        "\n",
        "\n",
        "      \n",
        "      return post_process(result)\n",
        "  \n",
        "  def train(self, boards, solutions):\n",
        "    #essentially, what we want to do here is solve each board\n",
        "    #its better to make our probability initially really small so that\n",
        "    #runtime isn't dogshit (it will take eons to run if we go through every single possible path)\n",
        "    #then our loss function will be p = p * log (wP/wR) (thinking about adding here, see which one does better), \n",
        "    # where wP is the weighted precision and wR is the weighted recall (this is based on wordhunt scores, eg 800 for 5 letter words)\n",
        "    #we keep going until we go through all the boards and solutions\n",
        "    #afterwards, print our final p-value to see where it ended up\n",
        "    return\n",
        "  \n",
        "\n",
        "  def evaluate(self, answer, reference):\n",
        "    #this is where we calculate the loss function stuff\n",
        "    return"
      ],
      "metadata": {
        "id": "o_UOthuAT3C0"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Isaac: Dictionary-Based Solver"
      ],
      "metadata": {
        "id": "vSblfq6cGcR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Isaac():\n",
        "  def __init__(self, dictionary):\n",
        "    self.dictionary = dictionary.listed\n",
        "  \n",
        "\n",
        "  def dfs(self, word, tile, adjlist, visited, path, board):\n",
        "    if(path == len(word)):\n",
        "      return True\n",
        "    \n",
        "    for neighbor in adjlist[tile]:\n",
        "      if neighbor not in visited:\n",
        "        if board[neighbor[0]][neighbor[1]] == word[path]:\n",
        "          temp = visited.copy()\n",
        "          temp.append(neighbor)\n",
        "          if(self.dfs(word,neighbor, adjlist, temp, path + 1, board)):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "  def solve(self, board):\n",
        "    result = []\n",
        "    adjlist = board_to_adjlist(board)\n",
        "    lst = board_to_list(board)\n",
        "    ref = board_to_dict(board)\n",
        "    #im going to make a dictionary for easy access\n",
        "\n",
        "    for word in self.dictionary:\n",
        "      if len(word) > 2 and len(word) <= 16 and exists(word, lst):\n",
        "        for start in ref[word[0]]:\n",
        "          if(self.dfs(word, start, adjlist, [start], 1, board)):\n",
        "            result.append(word)\n",
        "            break\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "evQruf96TK1A"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Begins!"
      ],
      "metadata": {
        "id": "RRKSl8a9GWbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tester = [['H','A','M','R'], ['U','Y','E','A'], ['R','D','N','N'], ['L','L','E','K']]"
      ],
      "metadata": {
        "id": "mjlDHyzbVki1"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = Dictionary(\"dictionary.txt\",10000)"
      ],
      "metadata": {
        "id": "Rz-8XChF-zAv"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = LanguageModel(3, \"dictionary.txt\")\n",
        "albert = Albert(lm.model, 3)"
      ],
      "metadata": {
        "id": "t4CxuO3kSQud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tic = time.perf_counter()\n",
        "result = albert.solve(tester)\n",
        "result = validate(dictionary, result)\n",
        "toc = time.perf_counter()\n",
        "print(toc - tic)\n",
        "result = sorted(result, key = len, reverse = True)\n",
        "print(result)\n",
        "print(len(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZlroFIi8_dP",
        "outputId": "a353c801-c80c-4341-9188-ef096b55f596"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13382851499954995\n",
            "['REMANNED', 'MANNER', 'LENDER', 'ENDEAR', 'MANNED', 'YEANED', 'REMAND', 'AMEND', 'MANED', 'ENDER', 'DERMA', 'ARENE', 'REMAN', 'DENAR', 'RUDER', 'HYMEN', 'RAMEN', 'YAMEN', 'NAMED', 'ARMED', 'NAMER', 'LEND', 'YEAR', 'RAND', 'NAME', 'REDE', 'YEAN', 'MEND', 'DELL', 'MANE', 'DERM', 'DEAN', 'MARE', 'KNAR', 'DENE', 'REND', 'AMEN', 'HAME', 'REAM', 'RUDE', 'DEAR', 'KEN', 'DEN', 'MAN', 'ARM', 'DEL', 'RED', 'HAM', 'YEN', 'MED', 'YAM', 'AND', 'DRY', 'LED', 'EAR', 'NAM', 'RAM', 'MEN', 'ANE', 'END', 'NAN', 'ELL', 'ARE', 'RAN', 'AMA', 'MAR']\n",
            "66\n",
            "41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "isaac = Isaac(dictionary)"
      ],
      "metadata": {
        "id": "8y-59NsNSW2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tic = time.perf_counter()\n",
        "result = isaac.solve(tester)\n",
        "toc = time.perf_counter()\n",
        "result = sorted(result, key = len, reverse = True)\n",
        "print(toc- tic)\n",
        "print(result)\n",
        "print(len(result))\n",
        "long = [item for item in result if len(item)>3]\n",
        "print(len(long))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuGsQtga6NvA",
        "outputId": "6e8c8b4b-c84e-4fd8-c9a3-6daf781aa8e9"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4627254580009321\n",
            "['REMANNED', 'ENDEAR', 'HURDLE', 'HURLED', 'HYAENA', 'KENNED', 'LENDER', 'MANNED', 'MANNER', 'RANKED', 'REDENY', 'REMAND', 'YEANED', 'YENNED', 'AMEND', 'ARENE', 'ARMED', 'DENAR', 'DERMA', 'DRYER', 'DYNEL', 'EDEMA', 'ELDER', 'EMYDE', 'ENDER', 'ENEMA', 'ENEMY', 'HAYED', 'HAYER', 'HYENA', 'HYMEN', 'KNELL', 'MANED', 'MAYED', 'MEANY', 'NAMED', 'NAMER', 'RAMEN', 'RANDY', 'REDRY', 'REMAN', 'RUDER', 'YAMEN', 'AMAH', 'AMEN', 'AREA', 'ARMY', 'DEAN', 'DEAR', 'DELL', 'DEMY', 'DENE', 'DENY', 'DERM', 'DYER', 'DYNE', 'EMYD', 'EYNE', 'HAED', 'HAEM', 'HAEN', 'HAME', 'HURL', 'KNAR', 'LEND', 'MANE', 'MANY', 'MARE', 'MAUD', 'MEAN', 'MEND', 'MYNA', 'NAME', 'NEAR', 'NEMA', 'NENE', 'RAND', 'RANK', 'REAM', 'REDE', 'REND', 'RUDE', 'RYND', 'YAUD', 'YEAH', 'YEAN', 'YEAR', 'AMA', 'AND', 'ANE', 'ANY', 'ARE', 'ARM', 'AYE', 'DEL', 'DEN', 'DEY', 'DRY', 'DUH', 'DYE', 'EAR', 'EAU', 'ELD', 'ELL', 'END', 'ERA', 'HAE', 'HAM', 'HAY', 'KEN', 'LED', 'LEK', 'MAE', 'MAN', 'MAR', 'MAY', 'MED', 'MEN', 'NAE', 'NAM', 'NAN', 'RAM', 'RAN', 'RED', 'REM', 'RYA', 'RYE', 'URD', 'YAH', 'YAM', 'YEA', 'YEN']\n",
            "132\n",
            "87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm.model[\"AYE\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72cHa5zSQU_C",
        "outputId": "e6a67e08-5660-45e1-b56c-17e836d411d9"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1490933512424446"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    }
  ]
}